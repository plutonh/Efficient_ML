<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Efficient Machine Learning Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Layout ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    body { padding-top:4rem; font-family:-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; counter-reset:cite-counter;}
    main { max-width:1600px; margin:auto; }

    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Î©îÏù∏ Í∏∞ÏÇ¨(900 px)Î•º ÌôîÎ©¥ Ï§ëÏïôÏóê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .content-wrap{
      /* Í∏∞ÏÇ¨ Î∞ïÏä§Î•º Ï†ïÏ§ëÏïôÏóê! */
      width:900px;
      margin:0 auto;
      position:relative;        /* #toc Ï†àÎåÄÏ¢åÌëú Í∏∞Ï§Ä */
    }
    
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Î≥∏Î¨∏(Í∏∞ÏÇ¨) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .content-wrap article{
      width:100%;               /* Ï¶â 900 px */
      box-sizing:border-box;
      padding-left:1.25rem;     /* ÏÑ∏Î°úÏÑ†Í≥º Î≥∏Î¨∏ Í∞ÑÍ≤© */
      text-align:justify;
      text-justify:inter-word;
    }
    
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOC Ìå®ÎÑê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    #toc{
      /* Îç∞Ïä§ÌÅ¨ÌÜ±: Í∏∞ÏÇ¨ ÏôºÏ™ΩÏóê Í≥†Ï†ï */
      position:absolute;
      right:calc(100% + 2rem);  /* Í∏∞ÏÇ¨ ÏôºÏ™ΩÏúºÎ°ú 2 rem Í∞ÑÍ≤© */
      top:0;
      width:260px;
    
      /* ÏãúÍ∞ÅÏ†Å Ïä§ÌÉÄÏùº */
      border-right:2px solid #e5e5e5;
      padding-left:2rem;
      padding-right:0.5rem;
      margin-top:1.5rem;
      max-height:calc(100vh - 100px);
      overflow-y:auto;
    }
    
    #toc h2{ font-size:1.5rem; font-weight:700; margin-bottom:1rem; }
    #toc ul{ list-style:none; margin:0; padding:0; }
    #toc li{ margin-bottom:0.25rem; }
    #toc li>a{ text-decoration:none; color:#0d6efd; }
    #toc li>a:hover{ text-decoration:underline; }
    #toc ul ul{ margin-left:1rem; border-left:2px solid #e5e5e5; padding-left:0.75rem; }
    #toc a.toc-title{ font-size:1.4rem; font-weight:800; display:block; text-align:center; }
    #toc li > a{ color:#000; }
    #toc li > a:hover{ color:#000; text-decoration:underline; }
    
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Î™®Î∞îÏùº(Ìè≠ 1500 px Ïù¥Ìïò) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    @media (max-width:1500px){
      #toc{
        position:static;        /* ÌùêÎ¶ÑÏóê Îî∞Îùº ÏúÑÎ°ú */
        right:auto;
        border-right:none;
        padding-left:1rem;
        margin-top:0;
      }
      .content-wrap{
        width:100%;             /* Í∏∞ÏÇ¨ Ìè≠ Ïú†Îèô */
        padding:0 5%;           /* Ï¢å¬∑Ïö∞ Ïó¨Î∞± */
      }
    }
    
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Citation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .cite{
      counter-increment:cite-counter;
      text-decoration:none;
      color:#0d6efd;
      cursor:pointer;
      position:relative;
      top:-0.3em;
      font-size:0.8em;
    }
    .cite::after{
      content:"[" counter(cite-counter) "]";
    }

    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    article h1, article h2, article h3{ font-weight:800; margin-top:3rem; margin-bottom:1rem; }
    article p{ line-height:1.6; }

    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    article h1, article h2, article h3{
      font-weight:800; margin-top:3rem; margin-bottom:1rem;
    }
    
    article p{
      line-height:1.6;
      text-align:justify;      /* ‚Üê ÏñëÏ™Ω Ï†ïÎ†¨ */
      text-justify:inter-word;
    }

    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hero ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .hero-section{
      max-width:800px; 
      margin: 0 auto 2.5rem auto;
    }
    
    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Meta ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .meta-section{
      max-width:1000px;             
      margin:0 200px 2.5rem 350px;    
      padding:3rem 0 2.5rem 0;
    }

    .meta-head{ font-size:0.75rem; letter-spacing:0.05em; text-transform:uppercase; color:#6c757d; margin-bottom:0.5rem; }
    .meta-section li{ list-style:none; margin:0; }

    /* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Reference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
    .ref-title{
      display:block;             
      font-weight:700;
      color:#495057;            
    }
    
    .ref-detail{
      display:block;            
      color:#adb5bd;            
    }

    .w-20{ width:20% !important; }
    .w-30{ width:30% !important; }
    .w-40{ width:40% !important; }
    .w-60{ width:60% !important; }
    .w-70{ width:70% !important; }
    .w-80{ width:80% !important; }
    .w-90{ width:90% !important; }

    .eq-big{ font-size:0.8rem; }
    .eq-block{
      text-align:center;
      margin:1.5rem 0;
    }
    .eq-wrap{ display:inline-block; }
    .eq-block .cite{
      display:inline-block;
      vertical-align:top; 
      margin-left:0.4rem;
    }
    .fig-label{ display:block; font-weight:600; }

    .figure-img {
      margin-top: 3.0rem;    
      margin-bottom: 3.0rem;
    }

    .img-fluid {
      margin-top: 3.0rem;   
      margin-bottom: 3.0rem; 
    }
    
    .mt-cap{
      margin-top: 2.0rem;   /* ÌïÑÏöî Í∏∏Ïù¥Î°ú Ï°∞Ï†ï */
    }

    .mb-cap{
      margin-bottom: 2.0rem;   /* ÌïÑÏöî Í∏∏Ïù¥Î°ú Ï°∞Ï†ï */
    
    .table-note{
      display:inline-block;   /* figure Ï§ëÏïô Í∏∞Ï§ÄÏúºÎ°ú ‚ÄòÎ∞ïÏä§‚Äô ÌïòÎÇò */
      max-width:100%;         /* Ìëú¬∑Í∑∏Î¶º Ìè≠ ÎßåÌÅºÎßå               */
      text-align:left;        /* Î∞ïÏä§ ÎÇ¥Î∂ÄÎßå ÏôºÏ™Ω Ï†ïÎ†¨            */
      font-size:0.85rem;      /* Í≤ÄÏùÄ Í∏ÄÏî® ÏûëÏùÄ ÌÅ¨Í∏∞             */
      color:#000;
    }

  </style>
</head>
<body>
  <!-- Upper Navibar -->
  <nav class="navbar navbar-light bg-light fixed-top shadow-sm">
    <div class="container-fluid">
      <a class="navbar-brand fw-bold" href="#">
        2025 Efficient ML Systems Final Project
      </a>
    </div>
  </nav>

  <main class="my-4">

    <!-- Hero -->
    <section class="hero-section">
      <h1 class="display-4 fw-bold text-center">Efficient GNN with Transformer in Gate Sizing</h1>
    </section>

    <hr class="section-divider">

    <!-- Meta -->
    <section class="meta-section">
      <div class="row gy-4">
        <!-- Authors -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Authors</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>Jinmo Ahn</li>
            <li>Myungjun Kook</li>
            <li>Jonghyeon Nam</li>
          </ul>
        </div>
    
        <!-- Affiliations -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Affiliations</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>EE, POSTECH</li>
            <li>EE, POSTECH</li>
            <li>GSST, POSTECH</li>
          </ul>
        </div>
    
        <!-- Published -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Published</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>May&nbsp;30, 2025</li>
          </ul>
        </div>
      </div>
    </section>

    <hr class="section-divider">

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main & TOC ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <div class="content-wrap">

      <!-- TOC -->
      <aside id="toc">
        <h2>Contents</h2>
        <ul>
          <li>
            <a href="#background">Background</a>
            <ul>
              <li><a href="#background_1">Gate Sizing Problems</a></li>
              <li><a href="#background_2">Graph Representation</a></li>
              <li><a href="#background_3">Machine Learning Framework with GNN-based Embedding</a></li>
              <li><a href="#background_4">Previous ML-based Gate Sizer vs. Ours</a></li>
            </ul>
          </li>
          
          <li><a href="#methods">Methods</a>
            <ul>
              <li><a href="#methods_1">Why Heterogeneous Graph?</a></li>
              <li><a href="#methods_2">Graph Representation and Feature Embedding</a></li>
              <li><a href="#methods_3">Model Architecture (HeteroGAT + Transformer)</a></li>
              <li><a href="#methods_4">Loss Function</a></li>
            </ul>
          </li>
          
          <li><a href="#experiment_result">Experimental Results</a>
            <ul>
              <li><a href="#experiment_result_1">Experiment Setup</a></li>
              <li><a href="#experiment_result_2">DPH-Sizer vs. Previous Models</a></li>
              <li><a href="#experiment_result_3">Runtime Analysis</a></li>
            </ul>
          </li>
        
          <li><a href="#conclusion">Conclusion</a></li>
        
          <li><a href="#references">References</a></li>
        </ul>
      </aside>

      <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main article ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
      <article id="content">
        <!-- Each section gets an ID used in the TOC -->

        <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Background ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
        <section id="background">
          <h1>Background</h1>
          <hr class="section-divider">
          <p>
             Chip placement and routing (P&R) is the engineering task of designing the physical layout of a computer chip. 
            Although P&R can physically produce a layout through an optimization process, it is an NP-hard problem that requires optimization to consider various metrics for improvement.
            Particularly in the gate size optimization problem, researchers couldn‚Äôt find a feasible algorithmic method, even if they suggested various adequate methods for several decades. 
            Thus, machine learning (ML) has emerged as a promising direction for discovering more effective optimization strategies in gate sizing.
          </p>

          <p>
             We present an ML-driven approach that aims to solve the gate sizing problem with improved efficiency and scalability.
          </p>
        </section>
        
        <section id="background_1">
          <h3>Gate Sizing Problems</h3>
          <p>
             Gate sizing has long been explored as a technique for optimizing the performance, power, and area (PPA) of circuits by assigning the appropriate gate size to each cell from the available library candidates.
            For standard cells with identical functionality, increasing the cell size typically enhances drive strength and reduces intrinsic delay, but it also leads to higher leakage power and a larger area. 
          </p>

          <p>
             Thus, standard practice involves upsizing cells along timing-critical paths, while downsizing non-critical cells to optimize the overall design metrics. However, indiscriminately following this approach may cause problems because gate sizing decisions are not purely local.
            For example, upsizing a cell experiencing a setup timing violation can reduce its intrinsic delay and resolve downstream timing issues.
            Nevertheless, this upsizing simultaneously increases the cell‚Äôs input capacitance, imposing a higher load on its driver cell and thereby potentially increasing the delay of upstream stages.
            Consequently, this can introduce or exacerbate timing violations in preceding stages.
          </p>
          
          <p>
            Due to the combinatorial nature of gate sizing, it is inherently an NP-hard problem. 
            To address this complexity, a diverse range of methods has been proposed. 
            Traditional methods include geometric programming, greedy heuristics, and dynamic programming approaches.
            More recently, methods based on Lagrangian relaxation have emerged, demonstrating promising results by decomposing the global optimization problem into more tractable subproblems.
          </p>

          <p>
             However, as process technology continues to scale and circuit complexity increases significantly, these classical optimization methods have shown critical limitations.
            They typically suffer from excessive runtime or struggle to effectively explore the exponentially expanding search space, often yielding suboptimal performance in modern large-scale designs.
            Thus, most research attempts to devise efficient methods to overcome these limitations.
          </p>
        </section>

        <section id="background_2">
          
          <h3>Graph Representation</h3>
          <p>
             Graph representation methods are widely used when the connection information between components is essential, such as circuit design, social networks, and molecular modeling.
            Encoding the above data as graphs enables the structure to contain connectivity information, allowing learning-based methods, such as Graph Neural Networks (GNNs), to be effectively leveraged.
            For example, Google‚Äôs graph-based placement methodology
            <a class="cite" href="#ref-1" id="c-1"></a>
            successfully applied GNNs to netlist graphs for chip floorplanning.
          </p>

          <p>
             Motivated by these approaches, we adopt a graph-based representation of a circuit by viewing gate-level netlists as a graph with embedded features, enabling effective application of learning-based optimization techniques to the gate sizing problem.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/1_Method.png""
                 alt="Method"
                 class="figure-img img-fluid w-auto mb-auto">
            <figcaption class="figure-caption mt-3 mb-cap">
              Figure 1. Proposed Framework
            </figcaption>
          </figure>
          
          <p>
             To construct the graph representation of the gate-level netlist, we utilize GNN-based embedding as shown in Fig. 1, which explains the netlist to graph representation method flow.
            While various architectures such as GCN, GraphSAGE, and GAT could be employed for this purpose, we specifically adopt the GAT structure to address the over-smoothing problem commonly observed in deep neural networks.
          </p>
          
        </section>
        
        <section id="background_3">
          <h3>Machine Learning Framework with GNN-based Embedding</h4>
          <p>
             We formulate a machine learning framework that leverages GNN-based embeddings as input to predict performance-related metrics as output.
            Several prior works have explored ML-based gate sizing, such as NVIDIA‚Äôs transformer-based approach
            <a class="cite" href="#ref-3" id="c-3"></a>,
            which employs sequence modeling to predict gate sizes in an autoregressive manner, and RL-Sizer
            <a class="cite" href="#ref-4" id="c-4"></a>,
            which is the first to utilize GNNs for extracting states and actions within a Markov Decision Process.
            Another previous work by X. Zhou et al.
            <a class="cite" href="#ref-5" id="c-5"></a>,
            integrated a heterogeneous graph-based GNN into a Lagrangian relaxation-based sizing framework, using AI to narrow down the sizing candidates rather than performing direct sizing.
            Additionally, their approach utilizes a GNN based on neighborhood aggregation, which inherently limits the model's ability to capture only short-range information.
            This method thus faces limitations in adequately modeling long-range dependencies. On the other hand, DAG-Sizer
            <a class="cite" href="#ref-6" id="c-6"></a>
            represented circuits as homogeneous graphs and directly performed gate sizing through graph convolutional network (GCN) layers.
            Similarly, the local aggregation mechanism of GCN inherently restricts effective modeling of long-range dependencies.
            In contrast, TransSizer [citation] formulates gate sizing as sequence modeling using Transformers, which excel at modeling long-range dependencies.
            However, their locality-agnostic nature [citation] presents a significant drawback, as interactions between adjacent cells play a crucial role in gate sizing decisions.
          </p>

          <p>
             To address the challenges of the aforementioned ML-based model, we propose a transformer-based framework that utilizes GNN-derived embeddings.
            Our method efficiently compensates for locality agnosticism, a well-known shortcoming of transformers, by leveraging the GNN's ability to encode local structural information through message passing.
          </p>
        </section>

        <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Methods ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
        <section id="methods">
          <h1>Methods</h1>
          <hr class="section-divider">
          <p>
            In this section, we introduce our gate sizing framework, which leverages a heterogeneous graph attention network combined with transformer encoder layers.
            Our novel gate sizing method comprises two parts: a graph representation using Graph Attention Transformer (GAT) and a transformer structure with a suitable loss function for gate sizing.
            Additionally, we propose an efficient representation method for the netlist using our modified heterogeneous graph structure.
            We begin this section by explaining the motivation behind using a heterogeneous graph, which serves as the foundation for the subsequent components of our framework. 
          </p>
        </section>

        <section id="methods_1">
          <h3>Why Heterogeneous Graph?</h4>
          <p>
             In gate sizing, there are two common ways to represent all pins using a homogeneous graph.
            The first method involves concatenating all pin information into a single embedding for each cell and then performing inference on the cell.
            The second method connects pins and cells separately and masks the pins during inference.
            Fig. 2 below illustrates the comparison of the two approaches.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/2_WhyHetero.png""
                 alt="Why_Hetero"
                 class="figure-img img-fluid w-100 mb-auto">
            <figcaption class="figure-caption mt-3 mb-cap">
              Figure 2. Why is Heterogeneous Graph Representation Needed?
            </figcaption>
          </figure>

          <p>
             The first homogeneous graph method, where all pins are concatenated into the cell embedding, poses challenges due to the varying number of pins for each cell.
            To handle this, padding is applied to ensure consistent embedding dimensions across cells.
            However, as the number of pins per cell increases or decreases, this padding can become inefficient, potentially resulting in excessively large padding dimensions.
            This can make learning more difficult for the model, especially considering the challenge of determining how to handle the topological order of the pins.
            From an implementation perspective, this introduces an additional layer of complexity.
          </p>

          <p>
            On the other hand, the masking approach, while promising, has limitations in the context of gate sizing.
            Previous studies, including those based on DAG models, have highlighted the negative impact of excessive masking on model performance.
            Masking may obscure meaningful relationships between cells and pins, particularly when dealing with complex circuits, such as those involved in gate sizing.
          </p>

          <p>
             When it comes to sequences, homogeneous graphs introduce even greater challenges.
            Not only do they fail to capture all adjacent cell interactions, but the aforementioned issues with pin embeddings also persist.
            This results in an incomplete representation, making it challenging to achieve optimal gate sizing.
          </p>

          <p>
             Gate sizing is influenced by all neighboring cells, and representing only a subset of pins or focusing solely on the worst-case scenario would not logically capture the true impact on performance.
            A heterogeneous graph, however, naturally addresses these issues.
            By using distinct node types for pins and cells, a heterogeneous graph provides a more flexible and intuitive separation of components, allowing for more accurate modeling without the need for complex padding or excessive masking techniques.
          </p>
        </section>

        <section id="method_2">
          <h3>Graph Representation and Feature Embedding</h4>
          <p>
              Still, how we represent a netlist as a heterogeneous graph is an essential issue for the performance of prediction models.
            When we represent a netlist as a graph, representing every cell and pin in the circuit as a single graph requires substantial data and can lead to GPU memory issues.
            To alleviate computational complexity and enable efficient modeling, we first partition the circuit into smaller subcircuits.
            Each subcircuit is defined by selecting input ports or registers as sources and output ports or registers as sinks, encompassing all paths connecting these sources to their corresponding sinks.
            We then construct a heterogeneous graph that explicitly distinguishes between pins and cells, capturing multiple edge types to represent their connectivity.
            The equation of the graph representation is as follows:
          </p>
          
          <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Graph Definition Equation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
          <div class="eq-block">
            <span class="eq-wrap">
              \[
                {\Large
                  \mathcal{G}_{s,t}
                  \;=\;
                  \bigl(V_{s,t},\,E_{s,t}\bigr)
                  \;=\;
                  \Bigl(
                    \;
                    \bigcup_{p \in \mathcal{P}_{s,t}} V_{p},
                    \;
                    \bigcup_{p \in \mathcal{P}_{s,t}} E_{p}
                  \Bigr),
                }
              \]
          
              \[
                {\Large
                  V_{s,t}
                  \;=\;
                  V^{\text{cell}}_{s,t}
                  \;\cup\;
                  V^{\text{pin}}_{s,t},
                  \qquad
                  E_{s,t}
                  \;=\;
                  E^{\text{gate}}_{s,t}
                  \;\cup\;
                  E^{\text{net}}_{s,t}
                  \;\cup\;
                  E^{\text{cell}}_{s,t}
                }
              \]
            </span>
          </div>

          <p>
             where \( \mathcal{P}_{s,t} \) denotes the set of all paths originating at source node \( s \) and terminating at sink node \( t \).  
            \( \mathcal{G}_{s,t} \) is a heterogeneous graph with source node \( s \) and sink node \( t \).  
            The node set \( V_{s,t} \) is composed of two types of nodes: cell nodes \( V^{\text{cell}}_{s,t} \) and pin nodes \( V^{\text{pin}}_{s,t} \).  
            Similarly, the edge set \( E_{s,t} \) consists of three types of edges.  
            The first edge type is the gate-arc edge \( E^{\text{gate}}_{s,t} \), which is the connection between a cell and its corresponding pins.  
            During the gate sizing process, subtle changes of pins, such as the slew and arrival time, can significantly influence size decisions for the corresponding cell.  
            In order to capture this relationship, \( E^{\text{gate}}_{s,t} \) is introduced.  
            The second edge type is the net-arc edge \( E^{\text{net}}_{s,t} \), representing connections between pins belonging to different cells.  
            These edges correspond to physical net connections, capturing the driver-load relationships and effectively modeling signal propagation across the circuit.  
            For instance, increasing the size of a load cell increases its input-pin capacitance, potentially causing the driver‚Äôs output pin to violate its maximum-capacitance constraint.  
            Finally, the cell-arc edge \( E^{\text{cell}}_{s,t} \) represents internal connections within a cell.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/3_HeteroGraph.png""
                 alt="Hetero_Graph"
                 class="figure-img img-fluid w-70 mb-auto">
            <figcaption class="figure-caption mt-3 mb-cap">
              Figure 3. Netlist to Heterogeneous Graph Representation.
            </figcaption>
          </figure>

          <p>
             Fig. 3 above illustrates how our method modulates subcircuits and handles pin and gate nodes in a netlist-based manner.
            Three types of edges are represented as independent features and consist of two types of nodes.
            The netlist information is extracted in units of inter-register paths.
          </p>

          <figure style="text-align:center;">
            <figcaption class="figure-caption mt-cap mb-3">
              Table 1. Pin and Cell Node Features
            </figcaption>
            <img src="./Image/Table_1.png""
                 alt="Table_1"
                 class="img-fluid w-50 d-block mx-auto mt-auto mb-cap">
          </figure>

          <p>
             Also, we describe the features embedded in each cell and pin of the heterogeneous graph.
            Detailed information is provided in the Table.
            We extracted physical and timing information from the circuits and embedded these features into each pin and cell node.
            The features embedded in the cell nodes are denoted as ùëìcell, while those in the pin nodes are denoted as ùëìpin.
          </p>

        </section>

        <section id="methods_3">
          <h3>Model Architecture (HeteroGAT + Transformer)</h4>
          <p>
             Our proposed approach incorporates both topological and geometric circuit information to improve sizing performance.
            The following figure illustrates an overview of DPH-Sizer.
          </p>
          
          <figure style="text-align:center;">
            <img src="./Image/4_Framework.png""
                 alt="Framework"
                 class="figure-img img-fluid w-100 mb-auto">
            <figcaption class="figure-caption mt-3 mb-cap">
              Figure 4. Overview of our gate sizing optimization framework
            </figcaption>
          </figure>

          </p>
          <h4>Heterogeneous Graph</h5>
          <p>
             As we mentioned above, we explicitly represent netlist components (gate, pin) as separate nodes in a heterogeneous graph for gate sizing.
            This separation provides two key advantages.
            First, it effectively captures interactions between driver and receiver pins connected through nets; for instance, upsizing a cell increases the load on its driver cell‚Äôs output pin, impacting neighboring cells.
            Second, it allows the model to explicitly learn the relationship between cells and pins, such as variations in input pin capacitance and output drive strength due to cell resizing.
            Moreover, the GAT structure helps mitigate the over-smoothing problem.
            Another motivation for adopting GAT is that, in gate sizing, not all input cells are equally influential.
            Typically, the driver cell with the worst delay has the most significant impact on sizing decisions.
            Therefore, GAT, which assigns adaptive weights to neighbor nodes, is more suitable for gate sizing than GCN, which aggregates neighbors using fixed weights.
            The embedding update equation for the GAT is defined as follows:
          </p>

          <p>
            <div class="eq-block">
              <span class="eq-wrap">
                \[
                {\Large
                h'_{i}
                =\operatorname*{CONCAT}_{1 \le k \le K}
                \;\sigma\!\Bigl(
                  \sum_{j \in \mathcal{N}_{i}}
                  \alpha^{k}_{ij}\,\mathbf{h}_{j}\mathbf{W}_{k}
                \Bigr)
                }
                \]
              </span>
            </div>
            <div class="eq-block">
              <span class="eq-wrap">
                \[
                {\Large
                \alpha_{ij}
                =\operatorname*{softmax}_{j}(e_{ij})
                =\frac{\exp(e_{ij})}
                       {\displaystyle\sum_{k \in \mathcal{N}_{i}}\exp(e_{ik})}
                }
                \]
              </span>
            </div>
            <div class="eq-block">
              <span class="eq-wrap">
                \[
                {\Large
                e_{ij}
                =\sigma\!\Bigl(
                  \mathbf{a}^{\top}\bigl[\operatorname{CONCAT}(\tilde{\mathbf{h}}_{i},\tilde{\mathbf{h}}_{j})\bigr]
                \Bigr)
                }
                \]
              </span>
            </div>
          </p>

        </section>

          <p>
            Where \(e_{ij}\) denotes the attention score between node \(i\) and its neighbor node \(j\), computed using input embeddings \(\tilde{\mathbf{h}}_{i}\) and \(\tilde{\mathbf{h}}_{j}\) a learnable attention parameter vector \(\alpha\), 
            and the sigmoid activation function \(\sigma\), the normalized attention coefficient \(\alpha_{ij}\) is obtained by applying the softmax function to these attention scores, measuring the relative importance of node \(j\) in updating node \(i\)‚Äôs embedding. 
            The updated embedding \(\tilde{\mathbf{h}}_{i}\) for node \(i\) is calculated by concatenating the outputs from multiple attention heads, where each head ùëò employs a distinct learnable weight matrix \(\mathbf{W}_{k}\) and computes its attention coefficient \(\alpha^{k}_{ij}\). 
            Here, \(K\) represents the total number of attention heads, and \(\mathcal{N}_{i}\) denotes the set of neighbor nodes for node \(i\). 
            With bidirectional edges, cell nodes connect via cell-to-pin and pin-to-cell edges; final cell embeddings are obtained by attention-weighted summation of embeddings from each edge type. 
            However, pin nodes are connected through both cell-to-pin, pin-to-pin, and pin-to-cell edges. 
            Therefore, pin node embeddings are obtained by an attention-weighted sum of embeddings from three edge types.
          </p>

          <p>
          
          <h4>Transformer</h5>
          <p>
             Although the above GNN representation effectively captures the topological information of circuits, it primarily employs attention mechanisms focused on the local neighborhood of nodes.
            Consequently, it captures structural characteristics only within a relatively narrow scope.
            To address this limitation and incorporate global geometric structures and long-range dependencies within circuits, we integrate a transformer into our framework.
            However, transformers inherently lack the ability to capture positional information among elements unless such information is explicitly provided.
            In graph-based tasks, this is particularly important because the relationships between nodes are not inherently ordered, unlike in sequence-based tasks like NLP.
            Without positional encoding, the transformer is unable to differentiate among node types or capture their topological and spatial relationships within the graph.
          </p>

          <p>
            <div class="eq-big">
              \[
              {\Large
              \mathbf{X}' \;=\; \mathbf{X} \;+\; \mathbf{P}\!\bigl[\,:\,;\,L\,;\,:\,\bigr]
              }
              \]
            </div>
          </p>
            
          <p>
            Following two successive HeteroGAT layers, cell and pin embeddings individually reside in \( \mathbb{R}^{B \times N_c \times F_o} \) and \( \mathbb{R}^{B \times N_p \times F_o} \), respectively.
            We concatenate both embeddings to form the input to the positional encoder, fully leveraging the extracted node representations.
            Given an input embedding sequence \( \mathbf{X} \in \mathbb{R}^{B \times L \times d_{\text{model}}} \) and a learnable positional encoding matrix \( \mathbf{P} \in \mathbb{R}^{1 \times L_{\max} \times d_{\text{model}}} \), the output embedding \( \mathbf{X}' \) with positional encoding is defined by adding the positional encoding to the input embeddings.
            Here, \( B \) denotes the batch size, \( L = N_c + N_p \) is the total number of nodes, and \( d_{\text{model}} \) represents the embedding dimension.
            The positional encoding matrix \( \mathbf{P} \) is optimized jointly with the transformer parameters, enabling the model to learn meaningful positional information of the nodes adaptively.
          </p>
        </section>
        
        <section id="methods_3">
          <h3>Loss Function</h4>
          <p>
            We formulate the gate sizing problem as a node classification task.
            One challenge in formulating gate sizing as a classification task is that each gate has different size limits.
            This constraint sometimes causes the classifier‚Äôs output to exceed the maximum allowable size for a given gate.
            To address invalid predictions, we proposed loss modification.
            The binning method groups multiple possible gate sizes into a smaller number of bins, reducing the prediction range.
            Although this approach simplifies model training by reducing the number of output classes, it overlooks subtle differences among sizes within each bin, which hinders size optimization and may degrade the final model accuracy.
            In contrast, our approach directly penalizes invalid size predictions within the loss function, preserving detailed distinctions among individual cell sizes.
            Since timing-driven sizing optimization may inadvertently increase leakage power, we explicitly incorporate a leakage power term into our loss function to guide the model toward balanced optimization of both timing and leakage.
            The proposed loss function is defined as follows:
          </p>

          <div class="eq-block">
            <span class="eq-wrap">
              \[
                {\Large
                \mathrm{CE}_{\text{Loss_new}}
                \;=\;
                -\sum_{c=1}^{L}
                  \bigl(\mathrm{Leakage}_{i,c}
                        + \alpha \cdot M_{i,c}\bigr)
                  \,y_{i,c}\,
                  \log\!\bigl(q_{i,c}\bigr)
                }
              \]
          
              \[
                M_{i,c} \;=\;
                \begin{cases}
                  1, & \text{if gate size } c \text{ is invalid for gate } i,\\[4pt]
                  0, & \text{otherwise},
                \end{cases}
              \]
            </span>
          </div>
          
<!--           <p>
            <div class="eq-big">
              \[
              {\Large
              CE_{Loss_new}
              \;=\;
              - \sum_{c=1}^{L}
              \bigl(\mathrm{Leakage} + \mathrm{Penalty}\bigr)_{i,c}\;
              y_{i,c}\;
              \log\!\bigl(q_{i,c}\bigr)
              }
              \]
            </div>
          </p> -->

          <p>
            where \(L\) is the total number of available gate-size classes,
            \(y_{i,c}\in\{0,1\}\) is the one-hot label for gate \(i\),
            \(q_{i,c}\) is the softmax probability predicted by the model,
            and \(\mathrm{Leakage}_{i,c}\) denotes the leakage power
            of gate \(i\) when sized to class \(c\)
          </p>
          
          <p>
             By penalizing invalid size selections, the proposed loss function strongly discourages infeasible predictions during training.
            Additionally, by incorporating leakage power directly into the loss, the model naturally favors gate sizes with lower leakage when they provide similar accuracy.
          </p>
        </section>

        <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Experimental Results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
        <section id="experiment_result">
          <h1>Experimental Results</h1>
          <hr class="section-divider">

          <section id="experiment_result_1">
            <h3>Experiment Setup</h3>
            <p>
               The experiments are conducted on a server with an Intel Xeon Gold 6226R CPU and an NVIDIA TITAN RTX GPU.
              We utilize open-source benchmarks provided by OpenCores
              <a class="cite" href="#ref-7" id="c-7"></a>
              to train and evaluate the gate sizing model, as detailed in Table 2.
              The dataset comprises five training designs.
              For validation, one design is reserved for testing, while the remaining four are used for training.
              We used Synopsys Design Compiler
              <a class="cite" href="#ref-8" id="c-8"></a>
              for synthesis and Cadence Innovus
              <a class="cite" href="#ref-9" id="c-9"></a>,for placement with an Intel 22nm PDK.
              The gate sizing model is implemented using Pytorch and DGL.
            </p>

            <figure style="text-align:center;">
              <figcaption class="figure-caption mt-cap mb-3">
                Table 2. Design statistics for gate sizing.
              </figcaption>
              <img src="./Image/Table_2.png""
                   alt="Table_2"
                   class="img-fluid w-50 d-block mx-auto mt-auto mb-cap">
            </figure>
          </section>

          <section id="experiment_result_2">
          <h3>DPH-Sizer vs. Previous Models</h3>
            <p>
              Table 3 summarizes validation-set accuracy on previously unseen layouts, a direct indicator of each model‚Äôs representational capacity.
              We evaluate our own re-implementations of TransSizer (our reproduction achieves an average accuracy of 61.3%, essentially matching the 62% reported by the authors when the D5 benchmark is excluded), DAG-Sizer, a homogeneous-graph GAT baseline (HomoGAT), a HeteroGAT-only model, and the proposed HeteroGAT + Transformer architecture.
              For fairness, the parameter counts of ‚Äú\(Ours^1\)‚Äù and ‚Äú\(Ours^2\)‚Äù were kept within 5 % of each other by adjusting hidden-layer widths.
              All models were trained with an untuned cross-entropy loss, focusing solely on expressive power.
            </p>
  
            <figure class="my-4 text-center">
            
              <!-- Ìëú ÏÑ§Î™Ö : Í∞ÄÏö¥Îç∞ Ï†ïÎ†¨ + ÏúÑÏóê Ïó¨Î∞± -->
              <figcaption class="figure-caption mb-3">
                Table 3. Validation accuracy comparison of models. Best results are highlighted in bold.
              </figcaption>
            
              <!-- Ìëú Ïù¥ÎØ∏ÏßÄ -->
              <img src="./Image/Table_3.png"
                   alt="Table 3"
                   class="img-fluid w-50 d-block mx-auto mt-auto mb-auto">
            
              <!-- Ï£ºÏÑù : Ìëú ÏôºÏ™Ω Î™®ÏÑúÎ¶¨Ïóê ÎßûÏ∂§ -->
              <figcaption class="table-note">
                <sup>1</sup> HomoGAT‚ÄÉ<sup>2</sup> HeteroGAT‚ÄÉ<sup>3</sup> HeteroGAT + Transformer
              </figcaption>
            
            </figure>
  
            <p>
              The following are the key observations:
              <ul class="dash-list">
                <li>HomoGAT surpasses DAG-Sizer by roughly 5.8 % average accuracy, indicating that GAT layers are better suited than GCN layers for gate-sizing tasks.</li>
                <li>Using a heterogeneous graph (HeteroGAT) yields a 12.9 % accuracy gain over HomoGAT, confirming that explicitly separating pin and cell nodes captures circuit relationships more faithfully.</li>
                <li>Adding a Transformer on top of HeteroGAT delivers a further 6.8 % improvement, shoÏù¥Í±∞wing that the hybrid HeteroGAT + Transformer model provides the strongest representational capacity.</li>
              </ul>
            </p>
          </section>

          <section id="experiment_result_3">
            <h3>Runtime Analysis</h3>

            <figure style="text-align:center;">
              <figcaption class="figure-caption mt-cap mb-3">
                Table 4. Runtime comparison of models.
              </figcaption>
              <img src="./Image/Table_4.png""
                   alt="Table_4"
                   class="img-fluid w-40 d-block mx-auto mt-auto mb-cap">
            </figure>

            <p>
               Across the five benchmark designs, DPH-Sizer averages just 2.1 k ms per chip‚Äîabout 18 √ó faster than the original Transformer implementation (39.5 k ms).
              It still stays within roughly 7 √ó of the lightweight DAG heuristic (300.8 ms).
              Moreover, it delivers the highest validation accuracy at 95.5 % (versus 75.8 % for DAG and 57.2 % for Trans).
              Taken together, these results show that combining heterogeneous GAT layers with a Transformer backbone achieves state-of-the-art gate-sizing quality without prohibitive runtime overhead, making the method practical for modern design flows.
            </p>
          </section>
        </section>

        <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Conclusion ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
        <section id="conclusion">
          <h1>Conclusion</h1>
          <hr class="section-divider">
          <p>
            In this work, we formulate gate sizing as a node classification problem and propose DPH-Sizer, a novel framework that combines a heterogeneous graph, GAT, and a transformer network.
          </p>
          
          <p>
             Despite addressing several challenges of prior ML approaches, our proposed method still exhibits a few observable limitations:
            <ul class="dash-list">
              <li>Our model is trained on a limited set of technology nodes and design styles, which makes it harder for the model to learn when applied to unseen standard cell libraries for generalization.</li>
              <li>As our method generates a complete solution for all gates in a single inference step, it lacks support for iteration-based, incremental improvements.</li>
              <li> Since our model‚Äôs ultimate goal for our task is to serve as a prediction model, it is inherently bounded by the quality of the training data and cannot surpass the performance of the golden reference solutions.</li>
            </ul>
          </p>
          
          <p> 
            Therefore, the future direction of model enhancement is the quality of sizing solutions. 
            We directly modified the loss function, ultimately resulting in improved PPA metrics across all benchmarks.
          </p>
        </section>

        <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Reference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
        <section id="references">
          <h1>References</h1>
          <hr class="section-divider">
            <ol>

               <li id="ref-1">
                <a href="#c-1" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  A Graph Placement Methodology for Fast Chip Design
                </span>
                <span class="ref-detail">
                  A. Mirhoseini et al.,<em>Nature 594, pp. 207-212.</em> 2021.
                </span>
              </li>

              <li id="ref-2">
                <a href="#c-2" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  A Neural Architecture Predictor based on GNN-Enhanced Transformer
                </span>
                <span class="ref-detail">
                  X. Xiang et al., <em>Proc. AISTATS</em> 2024.
                </span>
              </li>

              <li id="ref-3">
                <a href="#c-3" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  TransSizer: A Novel Transformer-Based Fast Gate Sizer
                </span>
                <span class="ref-detail">
                  S. Nath et al., <em>Proc. ICCAD</em> 2022.
                </span>
              </li>
              
              <li id="ref-4">
                <a href="#c-4" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  RL-Sizer: VLSI gate sizing for timing optimization using deep reinforcement learning
                </span>
                <span class="ref-detail">
                  YC. Lu et al., <em>Proc. DAC</em> 2021.
                </span>
              </li>

              <li id="ref-5">
                <a href="#c-5" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  Heterogeneous Graph Neural Network-Based Imitation Learning for Gate Sizing Acceleration
                </span>
                <span class="ref-detail">
                  X. Zhou et al., <em>Proc. ICCAD</em> 2022.
                </span>
              </li>

              <li id="ref-6">
                <a href="#c-6" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  Heterogeneous Graph Neural Network-Based Imitation Learning for Gate Sizing Acceleration
                </span>
                <span class="ref-detail">
                  CK. Cheng et al., <em>ACM Transactions on Design Automation of Electronic Systems</em> vol. 28, no. 4, pp. 52:1-31, 2023.
                </span>
              </li>

              <li id="ref-7">
                <a href="#c-7" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  Opencores
                </span>
                <span class="ref-detail">
                  <em>https://opencores.org</em>
                </span>
              </li>

              <li id="ref-8">
                <a href="#c-8" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  Synopsys Design Compiler v18.06 User Guide
                </span>
              </li>

              <li id="ref-9">
                <a href="#c-9" class="back">‚Ü©Ô∏é</a>
                <span class="ref-title">
                  Cadence Innovus Implementation System v21.16 User Guide
                </span>
              </li>
              
           </ol>
        </section>

        
        
<!--    <section id="cfm-intuition"><h2>Intuition of Conditional Flow Matching</h2><p>‚ãØ</p></section>
        <section id="cfm-choices"><h2>Modelling Choices</h2><p>‚ãØ</p></section>
        <section id="cfm-uncond"><h2>From Conditional to Unconditional Velocity</h2><p>‚ãØ</p></section>

        <section id="further"><h1>Going Further</h1><p>‚ãØ</p></section>
        <section id="straight-flows"><h2>Fast Sampling with Straight Flows</h2><p>‚ãØ</p></section>
        <section id="diffusion"><h2>Diffusion Models</h2><p>‚ãØ</p></section>
        <section id="link"><h2>Link Between Diffusion and Flow-Matching</h2><p>‚ãØ</p></section>
        <section id="playground"><h2>CFM Playground</h2><p>‚ãØ</p></section> -->
        
      </article>
    </div>
  </main>
</body>
</html>
