<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Efficient Machine Learning Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /* ────────── Layout ────────── */
    body { padding-top:4rem; font-family:-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; }
    main { max-width:1600px; margin:auto; }

    /* Two-column grid (sidebar + content) */
    .content-wrap{ display:grid; grid-template-columns:260px 1fr; gap:3rem; }
    .content-wrap article{
      padding-right:16rem;    
      box-sizing:border-box;  
    }
    @media (max-width: 991.98px){
      .content-wrap{ grid-template-columns:1fr; }
      #toc{ position:static; }
    }

    /* ────────── TOC ────────── */
    #toc   { position:sticky; top:90px; max-height:calc(100vh - 100px); overflow-y:auto; margin-top:1.5rem; padding-right:0.5rem; padding-left:2rem; border-right:2px solid #e5e5e5;}
    #toc h2{ font-size:1.5rem; font-weight:700; margin-bottom:1rem; }
    #toc ul{ list-style:none; margin:0; padding:0; }
    #toc li{ margin-bottom:0.25rem; }
    #toc li>a{ text-decoration:none; color:#0d6efd; }
    #toc li>a:hover{ text-decoration:underline; }
    #toc ul ul{ margin-left:1rem; border-left:2px solid #e5e5e5; padding-left:0.75rem; }
    #toc a.toc-title{ font-size:1.4rem; font-weight:800; display:block; text-align:center; }
    #toc li > a{       color:#000; }
    #toc li > a:hover{ color:#000; text-decoration:underline; }

    @media (max-width:991.98px){
      #toc{ position:static; padding-left:1rem; border-right:none;}
    }

    /* ────────── Main ────────── */
    article h1, article h2, article h3{ font-weight:800; margin-top:3rem; margin-bottom:1rem; }
    article p{ line-height:1.6; }

    /* ────────── Hero ────────── */
    .hero-section{
      max-width:800px; 
      margin: 0 auto 2.5rem auto;
    }
    
    /* ────────── Meta ────────── */
    .meta-section{
      max-width:1000px;             
      margin:0 200px 2.5rem 350px;    
      padding:3rem 0 2.5rem 0;
    }

    .meta-head{ font-size:0.75rem; letter-spacing:0.05em; text-transform:uppercase; color:#6c757d; margin-bottom:0.5rem; }
    .meta-section li{ list-style:none; margin:0; }

    /* ────────── Reference ────────── */
    .ref-title{
      display:block;             
      font-weight:700;
      color:#495057;            
    }
    
    .ref-detail{
      display:block;            
      color:#adb5bd;            
    }

    .w-20{ width:20% !important; }
    .w-30{ width:30% !important; }
    .w-40{ width:40% !important; }
    .w-60{ width:60% !important; }
    .w-70{ width:70% !important; }
    .w-80{ width:80% !important; }

  </style>
</head>
<body>
  <!-- Upper Navibar -->
  <nav class="navbar navbar-light bg-light fixed-top shadow-sm">
    <div class="container-fluid">
      <span class="navbar-brand fw-bold">2025 Efficient ML Systems Final Project</span>
    </div>
  </nav>

  <main class="my-4">

    <!-- Hero -->
    <section class="hero-section">
      <h1 class="display-4 fw-bold text-center">Efficient GNN with Transformer in Gate Sizing</h1>
      <p class="lead">
        Conditional Flow Matching (CFM) was introduced by three concurrent papers at ICLR&nbsp;2023 —
        each framing the idea via conditional matching, rectifying flows, or stochastic interpolants.
        This article first reviews normalizing flows, then offers a comprehensive, visual exploration
        of CFM’s probabilistic foundations.
      </p>
    </section>

    <hr class="section-divider">

    <!-- Meta -->
    <section class="meta-section">
      <div class="row gy-4">
        <!-- Authors -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Authors</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>Jinmo Ahn</li>
            <li>Myeongjun Kook</li>
            <li>Jonghyeon Nam</li>
          </ul>
        </div>
    
        <!-- Affiliations -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Affiliations</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>EE, POSTECH</li>
            <li>EE, POSTECH</li>
            <li>GSST, POSTECH</li>
          </ul>
        </div>
    
        <!-- Published -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Published</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>May&nbsp;28, 2025</li>
          </ul>
        </div>
      </div>
    </section>

    <hr class="section-divider">

    <!-- ────────── Main & TOC ────────── -->
    <div class="content-wrap">

      <!-- TOC -->
      <aside id="toc">
        <h2>Contents</h2>
        <ul>
          <li>
            <a href="#background">Background</a>
            <ul>
              <li><a href="#background_1">Gate Sizing Problems</a></li>
              <li><a href="#background_2">Graph Representation Method</a></li>
              <li><a href="#background_3">Machine learning framework with GNN-based embedding</a></li>
            </ul>
          </li>
          
          <li><a href="#methods">Methods</a>
            <ul>
              <li><a href="#methods_1">Graph Representation and feature embedding</a></li>
              <li><a href="#methods_2">Model Architecture (HeteroGAT and Transformer)</a></li>
              <li><a href="#methods_3">Loss Function</a></li>
            </ul>
          </li>
          
          <li><a href="#experiment_result">Experimental Results</a>
            <ul>
              <li><a href="#experiment_result_1">Model Performance</a></li>
            </ul>
          </li>
        
          <li><a href="#conclusion">Conclusion</a></li>
        
          <li><a href="#references">References</a></li>
        </ul>
      </aside>

      <!-- ────────── Main article ────────── -->
      <article id="content">
        <!-- Each section gets an ID used in the TOC -->

        <!-- ────────── Background ────────── -->
        <section id="background">
          <h1>Background</h1>
          <hr class="section-divider">
          <p>
            Chip placement and routing (P&R) is the engineering task of designing the physical layout of a computer chip. 
            Although P&R could physically produce a layout with an optimization process, it is an NP-hard problem that can optimize the layout considering various metrics for improvement.
            Particularly in the gate size optimization problem, researchers couldn’t find a feasible algorithmic method, even if they suggested various adequate methods for several decades. 
            Thus, ML has emerged as a promising direction for discovering more effective optimization strategies in gate sizing.
          </p>

          <p>
            We present an ML-driven approach that aims to solve the gate sizing problem with improved efficiency and scalability.
          </p>
        </section>
        
        <section id="background_1">
          <h4>Gate Sizing Problems</h4>
          <p>
            Gate sizing has long been explored as a technique for optimizing the circuit's performance, power, and area (PPA) by assigning the appropriate gate size to each cell from the available library candidates.
            For standard cells with identical functionality, increasing the cell size typically enhances drive strength, reducing intrinsic delay but leading to higher leakage power and larger area. 
          </p>

          <p>
            Thus, standard practice involves upsizing cells along timing-critical paths, while downsizing non-critical cells to optimize the overall design metrics.
            However, indiscriminately following this approach may cause problems because gate sizing decisions are not purely local.
            For example, upsizing a cell experiencing a setup timing violation can reduce its intrinsic delay and resolve downstream timing issues.
            Nevertheless, this upsizing simultaneously increases the cell’s input capacitance, imposing a higher load on its driver cell and thereby potentially increasing the delay of upstream stages.
            Consequently, this can introduce or exacerbate timing violations in preceding stages.
          </p>

          <p>
            <ul class="dash-list">
              <li>Technical definition of gate sizing (with Figure)</li>
              <li>Explanation of PPA</li>
            </ul>
          </p>
          
          <p>
            As this sizing technique is one of the most powerful steps in achieving the target PPA in the optimization process, it has been extensively studied for a long time.
          </p>
        </section>

        <section id="background_2">
          <h4>Graph Representation Method</h4>
          <p>
            Graph representation methods are widely used when the connection information between components is essential, such as circuit design, social networks, and molecular modeling. 
            Encoding the above data as graphs enables the structure to contain connectivity information so that learning based methods, such as Graph Neural Networks (GNNs), can be effectively leveraged. 
            For example, Google’s graph-based placement methodology [Mirhoseini et al., Nature 2021] successfully applied GNNs to netlist graphs for chip floorplanning.

          </p>

          <img src="./Image/Method.png" alt="Method" class="img-fluid w-100 d-block mx-auto my-5">
          
          <p>
            Motivated by these approaches, we adopt a graph-based representation of a circuit by viewing gate-level netlists as a graph with embedded features, enabling practical application of learning-based optimization techniques to the gate sizing problem.
          </p>

          <p>
            To construct the graph representation of the gate-level netlist, we utilize GNN-based embedding. 
            While various architectures such as GCN, GraphSAGE, and GAT could be employed for this purpose, we specifically adopt the GAT structure to address the over-smoothing problem commonly observed in deep neural networks.
            For each K-head GAT layer, it can be formulated as follows:
          </p>

          <img src="./Image/Function_1.png" alt="Function_1" class="img-fluid w-50 d-block mx-auto my-5">

          <p>
            where h′_i and h_i are the old and new features of node v_i, ∥ denotes concatenation, αk_ij are normalized attention coefficients computed by the k-th attention mechanism and · _T denotes transposition, ω_k is the corresponding input linear transformation’s weight matrix, Ni denotes the neighbor of node v_i.
          </p>
          
        </section>
        
        <section id="background_3">
          <h4>Machine learning framework with GNN-based embedding</h4>
          <p>⋯ (write content here) ⋯</p>
        </section>

        <!-- ────────── Methods ────────── -->
        <section id="methods">
          <h1>Methods</h1>
          <hr class="section-divider">
          <p>
            In this section, we introduce our gate sizing framework, which leverages a heterogeneous graph attention network combined with transformer encoder layers.
            We explain how the netlist is represented as a graph with embedded features, detail the architecture used for size prediction, and describe the design of our loss function.
          </p>
        </section>

        <section id="methods_1">
          <h4>Graph Representation and feature embedding</h4>
          <p>
            We first partition the circuit into smaller subcircuits to simplify the circuit for efficient modeling.
            Each subcircuit is defined by designating input ports or registers as starting points and output ports or registers as endpoints. 
            Specifically, a subcircuit combines all paths sharing the same start and endpoints. 
            Thus, we propose a heterogeneous graph model that effectively captures the intrinsic structure of the circuit by delineating its two fundamental node types: pins and cells, as depicted in Fig. 2.
          </p>
          
          <img src="./Image/HeteroGraph.png" alt="Hetero_Graph" class="img-fluid w-70 d-block mx-auto my-5">

          <p>
            As shown in the figure, subcircuits were extracted by designating input ports or registers as the starting points and output ports or registers as the endpoints. 
            In addition, when we extracted subcircuits, each subgraph was defined as the combination of all paths that share the same start and endpoints. 
            These methods allow for better sizing by including connectivity information between cells that traditional sequence modeling does not have.
          </p>

          <p>
            Based on the extracted circuit information, nodes are assigned two types: pins and cells.
            Edges are classified into three categories according to their connectivity. 
            The first edge type is the gate arc edge, which is the connection between a cell and its corresponding pin.
            During the gate sizing process, subtle changes in the slew and arrival time of each pin can significantly influence size decisions for the corresponding cell.
            To capture this relationship, a gate arc edge is introduced. The second edge type is the net arc edge, which represents connections between pins of different cells.
            This edge is introduced because delays associated with net arcs directly impact gate sizing decisions.
            Finally, cell arc edge refers to pin connections within the same cell.
          </p>

          <img src="./Image/Table_1.png" alt="Table_1" class="img-fluid w-50 d-block mx-auto my-5">

          <p>
            Also, we describe the features embedded in each cell and pin of the heterogeneous graph.
            Detailed information is provided in the Table. 
            We extracted physical and timing information from the circuits and embedded these features into each pin and cell node.
            The features embedded in the cell nodes are denoted as 𝑓cell, while those in the pin nodes are denoted as 𝑓pin.
          </p>
        </section>

        <section id="methods_2">
          <h4>Model Architecture (HeteroGAT and Transformer)</h4>
          <p>
            Our proposed approach incorporates both topological and geometric circuit information to improve sizing performance.
            The following figure illustrates an overview of DPH-Sizer.
          </p>

          <img src="./Image/Flow.png" alt="Flow" class="img-fluid w-80 d-block mx-auto my-5">

          </p>
            <h5>Heterogeneous Graph</h5>
  
            Unlike prior approaches that treat pins and cells as a single entity, we explicitly represent them as separate nodes in a heterogeneous graph for gate sizing. 
            This separation provides two key advantages.
            First, it effectively captures interactions between driver and receiver pins connected through nets; for instance, upsizing a cell increases the load on its driver cell’s output pin, impacting neighboring cells.
            Second, it allows the model to explicitly learn the relationship between cells and pins, such as variations in input pin capacitance and output drive strength due to cell resizing.
            Moreover, as previously noted, GAT helps mitigate the over-smoothing problem.
            Another motivation for adopting GAT is that, in gate sizing, not all input cells are equally influential.
            Typically, the driver cell with the worst delay has the most significant impact on sizing decisions.
            Therefore, GAT, which assigns adaptive weights to neighbor nodes, is more suitable for gate sizing than GCN, which aggregates neighbors using fixed weights.
            The embedding update equation for the GAT is defined as follows:
          </p>
          
          <img src="./Image/Function_2.png" alt="Function_2" class="img-fluid w-40 d-block mx-auto my-5">

          <p>
            Where 𝑒_𝑖𝑗 denotes the attention score between node 𝑖 and its neighbor node 𝑗, computed using input embeddings˜_ℎ_𝑖 and˜_ℎ_𝑗, a learnable attention parameter vector 𝑎, and the sigmoid activation function 𝜎, the normalized attention coefficient 𝛼_𝑖𝑗 is obtained by applying the softmax function to these attention scores, measuring the relative importance of node 𝑗 in updating node 𝑖’s embedding. The updated embedding ℎ′𝑖 for node 𝑖is calculated by concatenating the outputs from multiple attention heads, where each head 𝑘 employs a distinct learnable weight matrix 𝑊𝑘 and computes its attention coefficient 𝛼𝑘_𝑖𝑗. Here, 𝐾 represents the total number of attention heads, and N𝑖 denotes the set of neighbor nodes for node 𝑖. 
            With bidirectional edges, cell nodes connect via cell-to-pin and pin-to-cell edges; final cell embeddings are obtained by attention-weighted summation of embeddings from each edge type. 
            However, pin nodes are connected through both cell-to-pin, pin-to-pin, and pin-to-cell edges. 
            Therefore, pin node embeddings are obtained by an attention-weighted sum of embeddings from three edge types.
          </p>

          <p>
            <h5>Transformer</h5>
            
            Although the above graph structure effectively captures the topological information of circuits, it primarily employs attention mechanisms focused on the local neighborhood of nodes. 
            Consequently, it captures structural characteristics only within a relatively narrow scope.
            To address this limitation and incorporate global geometric structures and long-range dependencies within circuits, we integrate a transformer into our framework.
            However, transformers inherently lack the ability to capture positional information among elements unless such information is explicitly provided.
            In graph-based tasks, this is particularly important because the relationships between nodes are not inherently ordered, unlike in sequence-based tasks like NLP.
            Without positional encoding, the transformer is unable to differentiate among node types or capture their topological and spatial relationships within the graph.
          </p>
          
          <img src="./Image/Function_3.png" alt="Function_3" class="img-fluid w-20 d-block mx-auto my-5">

          <p>
            Following two successive HeteroGAT layers, cell and pin embeddings individually reside in R^(𝐵×𝑁𝑐 ×𝐹𝑜) and R^(𝐵×𝑁𝑝 ×𝐹𝑜), respectively.
            We concatenate both embeddings to form the input to the positional encoder, fully leveraging the extracted node representations.
            Given an input embedding sequence 𝑋 ∈R^(𝐵×𝐿×𝑑_model) and a learnable positional encoding matrix 𝑃 ∈R^(1×𝐿_max × 𝑑_model), the output embedding 𝑋′with positional encoding is defined by adding the positional encoding to the input embeddings.
            Where 𝐵 denotes the batch size, 𝐿= 𝑁𝑐+𝑁𝑝 is the total number of nodes, and 𝑑model represents the embedding dimension.
            The positional encoding matrix 𝑃 is optimized jointly with the transformer parameters, enabling the model to learn meaningful positional information of the nodes adaptively.
          </p>
        </section>
        
        <section id="methods_3">
          <h4>Loss Function</h4>
          <p>
            We formulate the gate sizing problem as a node classification task.
            One challenge in formulating gate sizing as a classification task is that each gate has different size limits.
            This constraint sometimes causes the classifier’s output to exceed the maximum allowable size for a given gate.
            To address invalid predictions, we proposed loss modification.
            The binning method groups multiple possible gate sizes into a smaller number of bins, reducing the prediction range.
            Although this approach simplifies model training by decreasing the number of output classes, it ignores subtle differences among sizes within each bin, hindering size optimization and potentially degrading final model accuracy.
            In contrast, our approach directly penalizes invalid size predictions within the loss function, preserving detailed distinctions among individual cell sizes.
            Since timing-driven sizing optimization may inadvertently increase leakage power, we explicitly incorporate a leakage power term into our loss function to guide the model toward balanced optimization of both timing and leakage.
            The proposed loss function is defined as follows:
          </p>
          
          <img src="./Image/Function_4.png" alt="Function_4" class="img-fluid w-60 d-block mx-auto my-5">
          
          <p>
            By penalizing invalid size selections, the proposed loss function strongly discourages infeasible predictions during training.
            Additionally, by incorporating leakage power directly into the loss, the model naturally favors gate sizes with lower leakage when they provide similar accuracy.
          </p>
        </section>

        <!-- ────────── Experimental Results ────────── -->
        <section id="experiment_result">
          <h1>Experimental Results</h1>
          <hr class="section-divider">
          <p>
            <ul class="dash-list">
              <li>Explanation of metric (Model Performance, PPA, Runtime)</li>
              <li>Discussion about how the model approaches this performance</li>
            </ul>
          </p>
        </section>

        <section id="experiment_result_1">
          <h4>Model Performance</h4>
          <p>⋯ (write content here) ⋯</p>
        </section>

        <!-- ────────── Conclusion ────────── -->
        <section id="conclusion">
          <h1>Conclusion</h1>
          <hr class="section-divider">
          <p>
            In this work, we formulated gate sizing as a node classification problem and proposed DPH-Sizer, a novel framework combining a heterogeneous graph, Graph Attention Networks (GAT), and a transformer network.
            
            <ul class="dash-list">
              <li>Limitations of this work</li>
            </ul>
          
            Therefore, the future direction of model enhancement is the quality of sizing solutions. 
            We directly modified the loss function, ultimately resulting in improved PPA metrics across all benchmarks.
          </p>
        </section>

        <section id="conclusion_1">
          <p>⋯ (write content here) ⋯</p>
        </section>

        <!-- ────────── Reference ────────── -->
        <section id="references">
          <h1>References</h1>
          <hr class="section-divider">
            <ol>
              <li>
                <span class="ref-title">
                  Gate sizing in MOS digital circuits with linear programming
                </span>
                <span class="ref-detail">
                  MRCM. Berkelaar et al., <em>Proc. EDAC</em>, 1990.
                </span>
              </li>

              <li>
                <span class="ref-title">
                  Linear programming for sizing, Vth and Vdd assignment
                </span>
                <span class="ref-detail">
                  DG. Chinnery et al., <em>Proc. ISLPED</em>, 2005.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Gate Sizing For Cell Library-Based Designs
                </span>
                <span class="ref-detail">
                  S. Hu et al., <em>Proc. DAC</em>, 2007.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  A New Algorithm for Simultaneous Gate Sizing and Threshold Voltage Assignment
                </span>
                <span class="ref-detail">
                  Y. Liu et al., <em>Proc. ISPD</em>, 2009.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Gate sizing and device technology selection algorithms for high-performance industrial designs
                </span>
                <span class="ref-detail">
                  MM. Ozdal et al., <em>Proc. ICCAD</em>, 2011.
                </span>
              </li>
              <li>
                <span class="ref-title">
                  JiffyTune: circuit optimization using time-domain sensitivities
                </span>
                <span class="ref-detail">
                  G. Flach et al., <em>Proc. ISVLSI</em>, 2013.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Fast Lagrangian relaxation-based gate sizing using multi-threading
                </span>
                <span class="ref-detail">
                  A. Sharma et al., <em>Proc. ICCAD</em>, 2012.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Fast and exact simultaneous gate and wire sizing by Lagrangian relaxation
                </span>
                <span class="ref-detail">
                  CP. Chen et al., <em>in IEEE Transactions on Computer-Aided Design</em>, 1999.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Simultaneous Gate Sizing and Vth Assignment using Lagrangian Relaxation and Delay Sensitivities
                </span>
                <span class="ref-detail">
                  G. Flach et al., <em>Proc. ISVLSI</em>, 2013.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Effective Method for Simultaneous Gate Sizing and Vth Assignment Using Lagrangian Relaxation
                </span>
                <span class="ref-detail">
                  G. Flach et al., <em>in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, 2014.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Fast Lagrangian Relaxation-Based Multithreaded Gate Sizing Using Simple Timing Calibrations
                </span>
                <span class="ref-detail">
                  A. Sharma et al., <em>in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, 2020.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Task-Based Parallel Programming for Gate Sizing
                </span>
                <span class="ref-detail">
                  D. Mangiras et al., <em>in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, 2023.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Integrating LR Gate Sizing in an Industrial Place-and-Route Flow
                </span>
                <span class="ref-detail">
                  D. Chinnery et al., <em>Proc. ISPD</em> 2022.
                </span>
              </li>
              <li>
                <span class="ref-title">
                  RL-Sizer: VLSI gate sizing for timing optimization using deep reinforcement learning
                </span>
                <span class="ref-detail">
                  YC. Lu et al., <em>Proc. DAC</em> 2021.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Heterogeneous Graph Neural Network-Based Imitation Learning for Gate Sizing Acceleration
                </span>
                <span class="ref-detail">
                  X. Zhou et al., <em>Proc. ICCAD</em> 2022.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  AGD: A Learning-based Optimization Framework for EDA and its Application to Gate Sizing
                </span>
                <span class="ref-detail">
                  X. Zhou et al., <em>Proc. DAC</em> 2023.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Learning-driven Physically-aware Large-scale Circuit Gate Sizing
                </span>
                <span class="ref-detail">
                  Y. Ye et al., <em>in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, 2024.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  DAGSizer: A directed graph convolutional network approach to discrete gate sizing of VLSI graphs
                </span>
                <span class="ref-detail">
                  CK. Cheng et al., <em>ACM Transactions on Design Automation of Electronic Systems</em> 2023.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Improving timing &amp; power trade-off in post-place optimization using multi-agent reinforcement learning
                </span>
                <span class="ref-detail">
                  J. Seo et al., <em>Proc. ICCAD</em> 2024.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  TransSizer: A Novel Transformer-Based Fast Gate Sizer
                </span>
                <span class="ref-detail">
                  S. Nath et al., <em>Proc. ICCAD</em> 2022.
                </span>
              </li>
              <li>
                <span class="ref-title">
                  An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale
                </span>
                <span class="ref-detail">
                  A. Dosovitskiy et al., <em>Proc. ICLR</em> 2021.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  A Neural Architecture Predictor based on GNN-Enhanced Transformer
                </span>
                <span class="ref-detail">
                  X. Xiang et al., <em>Proc. AISTATS</em> 2024.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Attention is all you need
                </span>
                <span class="ref-detail">
                  Vaswani et al., <em>Proc. NeurIPS</em> 2017.
                </span>
              </li>
              <li>
                <span class="ref-title">
                  Understanding Graphs in EDA: From Shallow to Deep Learning
                </span>
                <span class="ref-detail">
                  Y. Ma et al., <em>Proc. ISPD</em> 2020.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Neural Message Passing for Quantum Chemistry
                </span>
                <span class="ref-detail">
                  K. Yang et al., <em>Proc. ICML</em> 2017.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  GraphSAGE: Inductive Representation Learning on Large Graphs
                </span>
                <span class="ref-detail">
                  W. Hamilton et al., <em>Proc. NeurIPS</em> 2017.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  DeepWalk: Online Learning of Social Representations
                </span>
                <span class="ref-detail">
                  B. Perozzi et al., <em>Proc. KDD</em> 2014.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Gate sizing and device technology selection algorithms for high-performance industrial designs
                </span>
                <span class="ref-detail">
                  MM. Ozdal et al., <em>Proc. ICCAD</em> 2011.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Graph attention networks
                </span>
                <span class="ref-detail">
                  P. Velickovic et al., arXiv preprint arXiv:1710.10903 2017.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  ParaGraph: Layout Parasitics and Device Parameter Prediction using Graph Neural Networks
                </span>
                <span class="ref-detail">
                  H. Ren et al., <em>Proc. DAC</em> 2020.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  A Fast Learning-Driven Signoff Power Optimization Framework
                </span>
                <span class="ref-detail">
                  YC. Lu et al., <em>Proc. ICCAD</em> 2020.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  GRANNITE: Graph Neural Network Inference for Transferable Power Estimation
                </span>
                <span class="ref-detail">
                  Y. Zhang et al., <em>Proc. DAC</em> 2020.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Deep Residual Learning for Image Recognition
                </span>
                <span class="ref-detail">
                  K. He et al., <em>Proc. CVPR</em> 2016.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  AST: Audio Spectrogram Transformer
                </span>
                <span class="ref-detail">
                  Y. Gong et al., <em>Proc. Interspeech</em> 2021.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  SSAST: Self-Supervised Audio Spectrogram Transformer
                </span>
                <span class="ref-detail">
                  Y. Gong et al., <em>Proc. ICASSP</em> 2022.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers
                </span>
                <span class="ref-detail">
                  Z. Liu et al., <em>Proc. ECCV</em> 2022.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Transfuser: Imitation with Transformer-based Sensor Fusion for Autonomous Driving
                </span>
                <span class="ref-detail">
                  A. Mehta et al., <em>Proc. CoRL</em> 2022.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  PyTorch: An Imperative Style, High-Performance Deep Learning Library
                </span>
                <span class="ref-detail">
                  A. Paszke et al., <em>Proc. NeurIPS</em> 2019.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks
                </span>
                <span class="ref-detail">
                  M. Wang et al., <em>arXiv preprint arXiv:1909.01315</em>, 2019.
                </span>
              </li>

              <li>
                <span class="ref-title">
                  A Graph Placement Methodology for Fast Chip Design
                </span>
                <span class="ref-detail">
                  A. Mirhoseini et al.,<em>Nature 594, pp. 207-212.</em> 2021.
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Opencores
                </span>
                <span class="ref-detail">
                  <em>https://opencores.org</em>
                </span>
              </li>

              <li>
                <span class="ref-title">
                  Synopsys Design Compiler v18.06 User Guide
                </span>
              </li>
              
              <li>
                <span class="ref-title">
                  Cadence Innovus Implementation System v21.16 User Guide
                </span>
              </li>
           </ol>
        </section>

        
        
<!--    <section id="cfm-intuition"><h2>Intuition of Conditional Flow Matching</h2><p>⋯</p></section>
        <section id="cfm-choices"><h2>Modelling Choices</h2><p>⋯</p></section>
        <section id="cfm-uncond"><h2>From Conditional to Unconditional Velocity</h2><p>⋯</p></section>

        <section id="further"><h1>Going Further</h1><p>⋯</p></section>
        <section id="straight-flows"><h2>Fast Sampling with Straight Flows</h2><p>⋯</p></section>
        <section id="diffusion"><h2>Diffusion Models</h2><p>⋯</p></section>
        <section id="link"><h2>Link Between Diffusion and Flow-Matching</h2><p>⋯</p></section>
        <section id="playground"><h2>CFM Playground</h2><p>⋯</p></section> -->
        
      </article>
    </div>
  </main>
</body>
</html>
