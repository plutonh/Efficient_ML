<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Efficient Machine Learning Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /* ────────── Layout ────────── */
    body { padding-top:4rem; font-family:-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; counter-reset:cite-counter;}
    main { max-width:1600px; margin:auto; }

    /* Two-column grid (sidebar + content) */
    .content-wrap{ display:grid; grid-template-columns:260px 1fr; gap:2rem; }
    
    /* 본문 폭 고정 & 가운데 배치 */
    .content-wrap article{
      max-width:900px;
      margin:0 auto;
      padding:0 2rem;
      box-sizing:border-box;
    }
    
    @media (max-width: 991.98px){
      .content-wrap{ grid-template-columns:1fr; }
      #toc{ position:static; }
    }

    /* ────────── Citation ────────── */
    .cite{
      counter-increment:cite-counter;     
      text-decoration:none;               
      color:#0d6efd;                       
      cursor:pointer;
      position:relative;
      top:-0.3em;                         
      font-size:0.8em;
    }

    .cite::after{
      content:"[" counter(cite-counter) "]"; 
    }

    /* ────────── TOC ────────── */
    #toc   { position:sticky; top:90px; max-height:calc(100vh - 100px); overflow-y:auto; margin-top:1.5rem; padding-right:0.5rem; padding-left:2rem; border-right:2px solid #e5e5e5;}
    #toc h2{ font-size:1.5rem; font-weight:700; margin-bottom:1rem; }
    #toc ul{ list-style:none; margin:0; padding:0; }
    #toc li{ margin-bottom:0.25rem; }
    #toc li>a{ text-decoration:none; color:#0d6efd; }
    #toc li>a:hover{ text-decoration:underline; }
    #toc ul ul{ margin-left:1rem; border-left:2px solid #e5e5e5; padding-left:0.75rem; }
    #toc a.toc-title{ font-size:1.4rem; font-weight:800; display:block; text-align:center; }
    #toc li > a{       color:#000; }
    #toc li > a:hover{ color:#000; text-decoration:underline; }

    @media (max-width:991.98px){
      #toc{ position:static; padding-left:1rem; border-right:none;}
    }

    /* ────────── Main ────────── */
    article h1, article h2, article h3{ font-weight:800; margin-top:3rem; margin-bottom:1rem; }
    article p{ line-height:1.6; }

    /* ────────── Main ────────── */
    article h1, article h2, article h3{
      font-weight:800; margin-top:3rem; margin-bottom:1rem;
    }
    
    article p{
      line-height:1.6;
      text-align:justify;      /* ← 양쪽 정렬 */
      text-justify:inter-word;
    }

    /* ────────── Hero ────────── */
    .hero-section{
      max-width:800px; 
      margin: 0 auto 2.5rem auto;
    }
    
    /* ────────── Meta ────────── */
    .meta-section{
      max-width:1000px;             
      margin:0 200px 2.5rem 350px;    
      padding:3rem 0 2.5rem 0;
    }

    .meta-head{ font-size:0.75rem; letter-spacing:0.05em; text-transform:uppercase; color:#6c757d; margin-bottom:0.5rem; }
    .meta-section li{ list-style:none; margin:0; }

    /* ────────── Reference ────────── */
    .ref-title{
      display:block;             
      font-weight:700;
      color:#495057;            
    }
    
    .ref-detail{
      display:block;            
      color:#adb5bd;            
    }

    .w-20{ width:20% !important; }
    .w-30{ width:30% !important; }
    .w-40{ width:40% !important; }
    .w-60{ width:60% !important; }
    .w-70{ width:70% !important; }
    .w-80{ width:80% !important; }
    .w-90{ width:90% !important; }

    .eq-big{ font-size:0.8rem; }
    .eq-block{
      text-align:center;
      margin:1.5rem 0;
    }
    .eq-wrap{ display:inline-block; }
    .eq-block .cite{
      display:inline-block;
      vertical-align:top; 
      margin-left:0.4rem;
    }
    .fig-label{ display:block; font-weight:600; }

  </style>
</head>
<body>
  <!-- Upper Navibar -->
  <nav class="navbar navbar-light bg-light fixed-top shadow-sm">
    <div class="container-fluid">
      <a class="navbar-brand fw-bold" href="#">
        2025 Efficient ML Systems Final Project
      </a>
    </div>
  </nav>

  <main class="my-4">

    <!-- Hero -->
    <section class="hero-section">
      <h1 class="display-4 fw-bold text-center">Efficient GNN with Transformer in Gate Sizing</h1>
    </section>

    <hr class="section-divider">

    <!-- Meta -->
    <section class="meta-section">
      <div class="row gy-4">
        <!-- Authors -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Authors</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>Jinmo Ahn</li>
            <li>Myungjun Kook</li>
            <li>Jonghyeon Nam</li>
          </ul>
        </div>
    
        <!-- Affiliations -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Affiliations</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>EE, POSTECH</li>
            <li>EE, POSTECH</li>
            <li>GSST, POSTECH</li>
          </ul>
        </div>
    
        <!-- Published -->
        <div class="col-12 col-md-4">
          <div class="meta-head">Published</div>
          <ul class="list-unstyled ps-0 m-0">
            <li>May&nbsp;28, 2025</li>
          </ul>
        </div>
      </div>
    </section>

    <hr class="section-divider">

    <!-- ────────── Main & TOC ────────── -->
    <div class="content-wrap">

      <!-- TOC -->
      <aside id="toc">
        <h2>Contents</h2>
        <ul>
          <li>
            <a href="#background">Background</a>
            <ul>
              <li><a href="#background_1">Gate Sizing Problems</a></li>
              <li><a href="#background_2">Graph Representation Method</a></li>
              <li><a href="#background_3">Machine Learning Framework with GNN-based Embedding</a></li>
            </ul>
          </li>
          
          <li><a href="#methods">Methods</a>
            <ul>
              <li><a href="#methods_1">Graph Representation and Feature Embedding</a></li>
              <li><a href="#methods_2">Model Architecture (HeteroGAT and Transformer)</a></li>
              <li><a href="#methods_3">Loss Function</a></li>
            </ul>
          </li>
          
          <li><a href="#experiment_result">Experimental Results</a>
        
          <li><a href="#conclusion">Conclusion</a></li>
        
          <li><a href="#references">References</a></li>
        </ul>
      </aside>

      <!-- ────────── Main article ────────── -->
      <article id="content">
        <!-- Each section gets an ID used in the TOC -->

        <!-- ────────── Background ────────── -->
        <section id="background">
          <h1>Background</h1>
          <hr class="section-divider">
          <p>
            Chip placement and routing (P&R) is the engineering task of designing the physical layout of a computer chip.
            Although P&R can physically produce a layout through an optimization process, it is an NP-hard problem that requires optimization to consider various metrics for improvement.
            Particularly in the gate size optimization problem, researchers couldn’t find a feasible algorithmic method, even if they suggested various adequate methods for several decades.
            Thus, ML has emerged as a promising direction for discovering more effective optimization strategies in gate sizing.
          </p>

          <p>
            We present an ML-driven approach that aims to solve the gate sizing problem with improved efficiency and scalability.
          </p>
        </section>
        
        <section id="background_1">
          <h3>Gate Sizing Problems</h3>
          <p>
            Gate sizing has long been explored as a technique for optimizing the circuit's performance, power, and area (PPA) by assigning the appropriate gate size to each cell from the available library candidates.
            For standard cells with identical functionality, increasing the cell size typically enhances drive strength, reduces intrinsic delay, but leads to higher leakage power and a larger area. 
          </p>

          <p>
            Thus, standard practice involves upsizing cells along timing-critical paths, while downsizing non-critical cells to optimize the overall design metrics.
            However, indiscriminately following this approach may cause problems because gate sizing decisions are not purely local.
            For example, upsizing a cell experiencing a setup timing violation can reduce its intrinsic delay and resolve downstream timing issues.
            Nevertheless, this upsizing simultaneously increases the cell’s input capacitance, imposing a higher load on its driver cell and thereby potentially increasing the delay of upstream stages.
            Consequently, this can introduce or exacerbate timing violations in preceding stages.
          </p>
          
          <p>
            Due to the combinatorial nature of gate sizing, it is inherently an NP-hard problem. 
            To address this complexity, a diverse range of methods has been proposed. 
            Traditional methods include geometric programming, greedy heuristics, and dynamic programming approaches.
            More recently, methods based on Lagrangian relaxation have emerged, demonstrating promising results by decomposing the global optimization problem into more tractable subproblems.
          </p>

          <p>
            However, as process technology continues to scale and circuit complexity increases significantly, these classical optimization methods have shown critical limitations.
            They typically suffer from excessive runtime or struggle to effectively explore the exponentially expanding search space, often yielding suboptimal performance in modern large-scale designs.
            To overcome these limitations, machine learning-based approaches have gained increasing attention.
          </p>

          <p>
            As this sizing technique is one of the most powerful steps in achieving the target PPA in the optimization process, it has been extensively studied for a long time.
          </p>
  
        </section>

        <section id="background_2">
          
          <h3>Graph Representation Method</h3>
          <p>
            Graph representation methods are widely used when the connection information between components is essential, such as circuit design, social networks, and molecular modeling. 
            Encoding the above data as graphs enables the structure to contain connectivity information, allowing learning-based methods, such as Graph Neural Networks (GNNs), to be effectively leveraged. 
            For example, Google’s graph-based placement methodology
            <a class="cite" href="#ref-1" id="c-1"></a>
            successfully applied GNNs to netlist graphs for chip floorplanning.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/Method.png""
                 alt="Method"
                 class="figure-img img-fluid w-100 mb-4">
            <figcaption class="figure-caption">
              Figure 1. Proposed Method  
            </figcaption>
          </figure>
          
          <p>
            Motivated by these approaches, we adopt a graph-based representation of a circuit by viewing gate-level netlists as a graph with embedded features, 
            enabling practical application of learning-based optimization techniques to the gate sizing problem.
          </p>

          <p>
            To construct the graph representation of the gate-level netlist, we utilize GNN-based embedding. 
            While various architectures such as GCN, GraphSAGE, and GAT could be employed for this purpose, we specifically adopt the GAT structure to address the over-smoothing problem commonly observed in deep neural networks.
            For each K-head GAT layer, it can be formulated as follows:
          </p>

          <p>
            <div class="eq-block">
              <span class="eq-wrap">
                \[
                  {\Large
                    h'_{i} = \bigg\Vert_{k=1}^{K}
                    \sigma\!\Bigl(\sum_{j\in\mathcal{N}_{i}\cup\{i\}}
                    \alpha^{k}_{ij}\,\omega^{k}h_{j}\Bigr)
                  }
                \]
              </span>
            </div>

            <div class="eq-block">
              <span class="eq-wrap">
                \[
                  {\Large
                    a^{k}_{ij} =
                    \frac{\exp\!\bigl(\operatorname{LReLU}(a^{\top}[\omega^{k}h_{i}\Vert\omega^{k}h_{j}]))}
                         {\displaystyle\sum_{k\in\mathcal{N}_{i}\cup\{i\}}
                          \exp\!\bigl(\operatorname{LReLU}(a^{\top}[\omega^{k}h_{i}\Vert\omega^{k}h_{k}]))}
                  }
                \]
              </span>
              <a class="cite" href="#ref-2" id="c-2"></a>
            </div>
          </p>
        </section>
        
        <section id="background_3">
          <h3>Machine Learning Framework with GNN-based Embedding</h4>
          <p>
            We formulate a machine learning framework that leverages GNN-based embeddings as input to predict performance-related metrics as output. 
            Several prior works have explored ML-based gate sizing, such as NVIDIA’s transformer-based approach
            <a class="cite" href="#ref-3" id="c-3"></a>,
            which employs sequence modeling to predict gate sizes in an autoregressive manner, and RL-Sizer
            <a class="cite" href="#ref-4" id="c-4"></a>,
            which is the first to utilize GNNs for extracting states and actions within a Markov Decision Process. 
            However, these methods face limitations due to their reliance on sequential representations or the constraints of reinforcement learning paradigms.
          </p>

          <p>
            To address the challenges of the above ML-based model, we propose a transformer-based framework using GNN-derived embeddings.
            Our method efficiently compensates for locality agnosticism, a well-known shortcoming of transformers, by leveraging the GNN's ability to encode local structural information through message passing.
          </p>
        </section>

        <!-- ────────── Methods ────────── -->
        <section id="methods">
          <h1>Methods</h1>
          <hr class="section-divider">
          <p>
            In this section, we introduce our gate sizing framework, which leverages a heterogeneous graph attention network combined with transformer encoder layers.
            Our novel gate sizing method consists of 2 parts, graph representation with GAT, and transformer structure with proper loss function for gate sizing.
            Additionally, we propose an efficient representation method for the netlist using our modified heterogeneous graph structure. 
          </p>
        </section>

        <section id="methods_1">
          <h3>Graph Representation and Feature Embedding</h4>
          <p>
            We first partition the circuit into smaller subcircuits to simplify the circuit for efficient modeling.
            Each subcircuit is defined by designating input ports or registers as starting points and output ports or registers as endpoints. 
            Specifically, a subcircuit combines all paths sharing the same start and endpoints. 
            Thus, we propose a heterogeneous graph model that effectively captures the intrinsic structure of the circuit by delineating its two fundamental node types: pins and cells, as depicted in Fig. 2.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/HeteroGraph.png""
                 alt="Hetero_Grap"
                 class="figure-img img-fluid w-70 mb-4"">
          
            <figcaption class="figure-caption">
              Figure 2. Netlist to Heterogeneous Graph Representation.
            </figcaption>
          </figure>

          <p>
            As shown in the figure, subcircuits were extracted by designating input ports or registers as the starting points and output ports or registers as the endpoints. 
            In addition, when we extracted subcircuits, each subgraph was defined as the combination of all paths that share the same start and endpoints. 
            These methods enable better sizing by incorporating connectivity information between cells, which traditional sequence modeling lacks.
          </p>

          <p>
            Based on the extracted circuit information, nodes are assigned two types: pins and cells.
            Edges are classified into three categories according to their connectivity. 
            The first edge type is the gate arc edge, which is the connection between a cell and its corresponding pin.
            During the gate sizing process, subtle changes in the slew and arrival time of each pin can significantly influence size decisions for the corresponding cell.
            To capture this relationship, a gate arc edge is introduced. The second edge type is the net arc edge, which represents connections between pins of different cells.
            This edge is introduced because delays associated with net arcs directly impact gate sizing decisions.
            Finally, cell arc edge refers to pin connections within the same cell.
          </p>

          <figure style="text-align:center;">
            <img src="./Image/Table_1.png""
                 alt="Table_1"
                 class="img-fluid w-50 d-block mx-auto mb-4">
            <figcaption class="figure-caption">
              Table 1. Pin and Cell Node Features
            </figcaption>
          </figure>

          <p>
            Also, we describe the features embedded in each cell and pin of the heterogeneous graph.
            Detailed information is provided in the Table. 
            We extracted physical and timing information from the circuits and embedded these features into each pin and cell node.
            The features embedded in the cell nodes are denoted as 𝑓cell, while those in the pin nodes are denoted as 𝑓pin.
          </p>
        </section>

        <section id="methods_2">
          <h3>Model Architecture (HeteroGAT and Transformer)</h4>
          <p>
            Our proposed approach incorporates both topological and geometric circuit information to improve sizing performance.
            The following figure illustrates an overview of DPH-Sizer.
          </p>
          
          <figure style="text-align:center;">
            <img src="./Image/Flow.png""
                 alt="Flow"
                 class="figure-img img-fluid w-100 mb-4">
          
            <figcaption class="figure-caption">
              Figure 3. Overview of our gate sizing optimization framework
            </figcaption>
          </figure>

          </p>
            <h4>Heterogeneous Graph</h5>
  
            Unlike prior approaches that treat pins and cells as a single entity, we explicitly represent them as separate nodes in a heterogeneous graph for gate sizing. 
            This separation provides two key advantages.
            First, it effectively captures interactions between driver and receiver pins connected through nets; for instance, upsizing a cell increases the load on its driver cell’s output pin, impacting neighboring cells.
            Second, it enables the model to explicitly learn the relationship between cells and pins, such as variations in input pin capacitance and output drive strength resulting from cell resizing.
            Moreover, as previously noted, GAT helps mitigate the over-smoothing problem.
            Another motivation for adopting GAT is that, in gate sizing, not all input cells are equally influential.
            Typically, the driver cell with the worst delay has the most significant impact on sizing decisions.
            Therefore, GAT, which assigns adaptive weights to neighbor nodes, is more suitable for gate sizing than GCN, which aggregates neighbors using fixed weights.
            The embedding update equation for the GAT is defined as follows:
          </p>

          <p>
            <div class="eq-big">
              \[
              {\Large
              h'_{i}
              =\operatorname*{CONCAT}_{1 \le k \le K}
              \;\sigma\!\Bigl(
                \sum_{j \in \mathcal{N}_{i}}
                \alpha^{k}_{ij}\,\mathbf{h}_{j}\mathbf{W}_{k}
              \Bigr)
              }
              \]
              
              \[
              {\Large
              \alpha_{ij}
              =\operatorname*{softmax}_{j}(e_{ij})
              =\frac{\exp(e_{ij})}
                     {\displaystyle\sum_{k \in \mathcal{N}_{i}}\exp(e_{ik})}
              }
              \]
              
              \[
              {\Large
              e_{ij}
              =\sigma\!\Bigl(
                \mathbf{a}^{\top}\bigl[\operatorname{CONCAT}(\tilde{\mathbf{h}}_{i},\tilde{\mathbf{h}}_{j})\bigr]
              \Bigr)
              }
              \]
            </div>
          </p>

          <p>
            Where 𝑒_𝑖𝑗 denotes the attention score between node 𝑖 and its neighbor node 𝑗, computed using input embeddings˜_ℎ_𝑖 and˜_ℎ_𝑗, a learnable attention parameter vector 𝑎, 
            and the sigmoid activation function 𝜎, the normalized attention coefficient 𝛼_𝑖𝑗 is obtained by applying the softmax function to these attention scores, 
            measuring the relative importance of node 𝑗 in updating node 𝑖’s embedding. 
            The updated embedding ℎ′𝑖 for node 𝑖is calculated by concatenating the outputs from multiple attention heads, where each head 𝑘 employs a distinct learnable weight matrix 𝑊𝑘 and computes its attention coefficient 𝛼𝑘_𝑖𝑗. 
            Here, 𝐾 represents the total number of attention heads, and N𝑖 denotes the set of neighbor nodes for node 𝑖. 
            With bidirectional edges, cell nodes connect via cell-to-pin and pin-to-cell edges; final cell embeddings are obtained by attention-weighted summation of embeddings from each edge type. 
            However, pin nodes are connected through both cell-to-pin, pin-to-pin, and pin-to-cell edges. 
            Therefore, pin node embeddings are obtained by an attention-weighted sum of embeddings from three edge types.
          </p>

          <p>
            <h4>Transformer</h5>
            
            Although the above graph structure effectively captures the topological information of circuits, it primarily employs attention mechanisms focused on the local neighborhood of nodes. 
            Consequently, it captures structural characteristics only within a relatively narrow scope.
            To address this limitation and incorporate global geometric structures and long-range dependencies within circuits, we integrate a transformer into our framework.
            However, transformers inherently lack the ability to capture positional information among elements unless such information is explicitly provided.
            In graph-based tasks, this is particularly important because the relationships between nodes are not inherently ordered, unlike in sequence-based tasks like NLP.
            Without positional encoding, the transformer is unable to differentiate among node types or capture their topological and spatial relationships within the graph.
          </p>

          <p>
            <div class="eq-big">
              \[
              {\Large
              \mathbf{X}' \;=\; \mathbf{X} \;+\; \mathbf{P}\!\bigl[\,:\,;\,L\,;\,:\,\bigr]
              }
              \]
            </div>
          </p>
            
          <p>
            Following two successive HeteroGAT layers, cell and pin embeddings individually reside in R^(𝐵×𝑁𝑐 ×𝐹𝑜) and R^(𝐵×𝑁𝑝 ×𝐹𝑜), respectively.
            We concatenate both embeddings to form the input to the positional encoder, fully leveraging the extracted node representations.
            Given an input embedding sequence 𝑋 ∈R^(𝐵×𝐿×𝑑_model) and a learnable positional encoding matrix 𝑃 ∈R^(1×𝐿_max × 𝑑_model), the output embedding 𝑋′with positional encoding is defined by adding the positional encoding to the input embeddings.
            Where 𝐵 denotes the batch size, 𝐿= 𝑁𝑐+𝑁𝑝 is the total number of nodes, and 𝑑model represents the embedding dimension.
            The positional encoding matrix 𝑃 is optimized jointly with the transformer parameters, enabling the model to learn meaningful positional information of the nodes adaptively.
          </p>
        </section>
        
        <section id="methods_3">
          <h3>Loss Function</h4>
          <p>
            We formulate the gate sizing problem as a node classification task.
            One challenge in formulating gate sizing as a classification task is that each gate has different size limits.
            This constraint sometimes causes the classifier’s output to exceed the maximum allowable size for a given gate.
            To address invalid predictions, we proposed loss modification.
            The binning method groups multiple possible gate sizes into a smaller number of bins, reducing the prediction range.
            Although this approach simplifies model training by decreasing the number of output classes, it ignores subtle differences among sizes within each bin, hindering size optimization and potentially degrading final model accuracy.
            In contrast, our approach directly penalizes invalid size predictions within the loss function, preserving detailed distinctions among individual cell sizes.
            Since timing-driven sizing optimization may inadvertently increase leakage power, we explicitly incorporate a leakage power term into our loss function to guide the model toward balanced optimization of both timing and leakage.
            The proposed loss function is defined as follows:
          </p>
          
          <p>
            <div class="eq-big">
              \[
              {\Large
              \mathrm{CE\,Loss}_{\text{new}}
              \;=\;
              - \sum_{c=1}^{L}
              \bigl(\mathrm{Leakage} + \mathrm{Penalty}\bigr)_{i,c}\;
              y_{i,c}\;
              \log\!\bigl(q_{i,c}\bigr)
              }
              \]
            </div>
          </p>
          
          <p>
            By penalizing invalid size selections, the proposed loss function strongly discourages infeasible predictions during training.
            Additionally, by incorporating leakage power directly into the loss, the model naturally favors gate sizes with lower leakage when they provide similar accuracy.
          </p>
        </section>

        <!-- ────────── Experimental Results ────────── -->
        <section id="experiment_result">
          <h1>Experimental Results</h1>
          <hr class="section-divider">
          <p>
            <ul class="dash-list">
              <li>Explanation of metric (Model Performance, PPA, Runtime)</li>
              <li>Discussion about how the model approaches this performance</li>
            </ul>
          </p>
        </section>

        <!-- ────────── Conclusion ────────── -->
        <section id="conclusion">
          <h1>Conclusion</h1>
          <hr class="section-divider">
          <p>
            In this work, we formulate gate sizing as a node classification problem and propose DPH-Sizer, a novel framework that combines a heterogeneous graph, Graph Attention Networks (GAT), and a transformer network.
            
            <ul class="dash-list">
              <li>Limitations of this work</li>
            </ul>
          
            Therefore, the future direction of model enhancement is the quality of sizing solutions. 
            We directly modified the loss function, ultimately resulting in improved PPA metrics across all benchmarks.
          </p>
        </section>

        <!-- ────────── Reference ────────── -->
        <section id="references">
          <h1>References</h1>
          <hr class="section-divider">
            <ol>

               <li id="ref-1">
                <a href="#c-1" class="back">↩︎</a>
                <span class="ref-title">
                  A Graph Placement Methodology for Fast Chip Design
                </span>
                <span class="ref-detail">
                  A. Mirhoseini et al.,<em>Nature 594, pp. 207-212.</em> 2021.
                </span>
              </li>

              <li id="ref-2">
                <a href="#c-2" class="back">↩︎</a>
                <span class="ref-title">
                  A Neural Architecture Predictor based on GNN-Enhanced Transformer
                </span>
                <span class="ref-detail">
                  X. Xiang et al., <em>Proc. AISTATS</em> 2024.
                </span>
              </li>

              <li id="ref-3">
                <a href="#c-3" class="back">↩︎</a>
                <span class="ref-title">
                  TransSizer: A Novel Transformer-Based Fast Gate Sizer
                </span>
                <span class="ref-detail">
                  S. Nath et al., <em>Proc. ICCAD</em> 2022.
                </span>
              </li>
              
              <li id="ref-4">
                <a href="#c-4" class="back">↩︎</a>
                <span class="ref-title">
                  RL-Sizer: VLSI gate sizing for timing optimization using deep reinforcement learning
                </span>
                <span class="ref-detail">
                  YC. Lu et al., <em>Proc. DAC</em> 2021.
                </span>
              </li>
           </ol>
        </section>

        
        
<!--    <section id="cfm-intuition"><h2>Intuition of Conditional Flow Matching</h2><p>⋯</p></section>
        <section id="cfm-choices"><h2>Modelling Choices</h2><p>⋯</p></section>
        <section id="cfm-uncond"><h2>From Conditional to Unconditional Velocity</h2><p>⋯</p></section>

        <section id="further"><h1>Going Further</h1><p>⋯</p></section>
        <section id="straight-flows"><h2>Fast Sampling with Straight Flows</h2><p>⋯</p></section>
        <section id="diffusion"><h2>Diffusion Models</h2><p>⋯</p></section>
        <section id="link"><h2>Link Between Diffusion and Flow-Matching</h2><p>⋯</p></section>
        <section id="playground"><h2>CFM Playground</h2><p>⋯</p></section> -->
        
      </article>
    </div>
  </main>
</body>
</html>
